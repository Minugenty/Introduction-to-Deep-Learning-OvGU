{"cells":[{"cell_type":"markdown","metadata":{"id":"gWjwDOYHfidh"},"source":["# Assignment  - Let the Tensors Board?"]},{"cell_type":"markdown","metadata":{"id":"LaguYerghlmX"},"source":["# Experimenting with tf.data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20102,"status":"ok","timestamp":1698081315099,"user":{"displayName":"Minu Genty","userId":"09261842457608424196"},"user_tz":-120},"id":"iYOfhmCThqmA","outputId":"0d6901a7-b416-4ef4-9921-5510f870ebc9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUeLHvJ9iIiI"},"outputs":[],"source":["import os\n","os.chdir(\"/content/drive/My Drive/Colab Notebooks\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDLZULPjiITH"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from datasets import MNISTDataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2320,"status":"ok","timestamp":1698087723135,"user":{"displayName":"Minu Genty","userId":"09261842457608424196"},"user_tz":-120},"id":"UXyb39M2iO8S","outputId":"d992d6ee-6176-4e9a-8df9-80ac4ed50341"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n","(60000, 28, 28)\n","(60000,)\n","(10000, 28, 28)\n","(10000,)\n"]}],"source":["(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","print(train_images.shape)\n","print(train_labels.shape)\n","print(test_images.shape)\n","print(test_labels.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1698087773124,"user":{"displayName":"Minu Genty","userId":"09261842457608424196"},"user_tz":-120},"id":"1e-icLBZiQB-","outputId":"e5d098ff-a415-4770-ed2d-d8bffba9cbac"},"outputs":[{"name":"stdout","output_type":"stream","text":["<_TensorSliceDataset element_spec=(TensorSpec(shape=(28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))>\n"]}],"source":["data = tf.data.Dataset.from_tensor_slices((train_images,train_labels))\n","print(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dncVIDx4iPuq"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"QoQfKPPoXPvv"},"source":["# Fail 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44085,"status":"ok","timestamp":1698087888992,"user":{"displayName":"Minu Genty","userId":"09261842457608424196"},"user_tz":-120},"id":"E10N3A6MW_fA","outputId":"1e684fa0-6374-428a-cacd-978991d948ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss: 193.80471801757812 Accuracy: 0.1484375\n","Loss: nan Accuracy: 0.1015625\n","Loss: nan Accuracy: 0.0625\n","Loss: nan Accuracy: 0.1015625\n","Loss: nan Accuracy: 0.078125\n","Loss: nan Accuracy: 0.09375\n","Loss: nan Accuracy: 0.0859375\n","Loss: nan Accuracy: 0.109375\n","Loss: nan Accuracy: 0.078125\n","Loss: nan Accuracy: 0.09375\n","Loss: nan Accuracy: 0.0546875\n","Loss: nan Accuracy: 0.1484375\n","Loss: nan Accuracy: 0.0859375\n","Loss: nan Accuracy: 0.1171875\n","Loss: nan Accuracy: 0.1015625\n","Loss: nan Accuracy: 0.0859375\n","Loss: nan Accuracy: 0.109375\n","Loss: nan Accuracy: 0.125\n","Loss: nan Accuracy: 0.078125\n","Loss: nan Accuracy: 0.1015625\n","Loss: nan Accuracy: 0.109375\n","Final test accuracy: 0.09799999743700027\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","# get the data\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","\n","def preprocess_images(images):\n","    return images.reshape(-1, 784).astype(np.float32) / 255\n","\n","\n","def preprocess_labels(labels):\n","    return labels.reshape(-1).astype(np.int32)\n","\n","\n","train_images = preprocess_images(train_images)\n","test_images = preprocess_images(test_images)\n","train_labels = preprocess_labels(train_labels)\n","test_labels = preprocess_labels(test_labels)\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n","#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n","\n","\n","# define the model first, from input to output\n","\n","# this is a super deep model, cool!\n","n_units = 100\n","n_layers = 8\n","w_range = 0.4\n","\n","# just set up a \"chain\" of hidden layers\n","# model is represented by a list where each element is a layer,\n","# and each layer is in turn a list of the layer variables (w, b)\n","\n","# first layer goes from n_input to n_hidden\n","w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n","                      name=\"w0\")\n","b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n","layers = [[w_input, b_input]]\n","\n","# all other hidden layers go from n_hidden to n_hidden\n","for layer in range(n_layers - 1):\n","    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n","                    name=\"w\" + str(layer+1))\n","    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n","    layers.append([w, b])\n","\n","# finally add the output layer\n","w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n","                    name=\"wout\")\n","b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n","layers.append([w_out, b_out])\n","\n","# flatten the layers to get a list of variables\n","all_variables = [variable for layer in layers for variable in layer]\n","\n","\n","def model_forward(inputs):\n","    x = inputs\n","    for w, b in layers[:-1]:\n","        x = tf.nn.relu(tf.matmul(x, w) + b)\n","    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n","\n","    return logits\n","\n","\n","lr = 0.1\n","train_steps = 2000\n","for step, (img_batch, lbl_batch) in enumerate(train_data):\n","    if step > train_steps:\n","        break\n","\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        logits = model_forward(img_batch)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=logits, labels=lbl_batch))\n","\n","    grads = tape.gradient(xent, all_variables)\n","    for grad, var in zip(grads, all_variables):\n","        var.assign_sub(lr*grad)\n","\n","    if not step % 100:\n","        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","test_preds = model_forward(test_images)\n","test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(acc))\n"]},{"cell_type":"markdown","metadata":{"id":"cLMicF3mo9hQ"},"source":["The reason for the low accuracy could be due to\n","\n","1.   Wrong range of weights\n","2.   Too many layers\n","\n","We tried to reduce the weight range from [-0.4, 0.4] to [-0.1, 0.1]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NquCZVbwXXjQ"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","# get the data\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","\n","def preprocess_images(images):\n","    return images.reshape(-1, 784).astype(np.float32) / 255\n","\n","\n","def preprocess_labels(labels):\n","    return labels.reshape(-1).astype(np.int32)\n","\n","\n","train_images = preprocess_images(train_images)\n","test_images = preprocess_images(test_images)\n","train_labels = preprocess_labels(train_labels)\n","test_labels = preprocess_labels(test_labels)\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n","#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n","\n","\n","# define the model first, from input to output\n","\n","# this is a super deep model, cool!\n","n_units = 100\n","n_layers = 8\n","w_range = 0.1\n","\n","# just set up a \"chain\" of hidden layers\n","# model is represented by a list where each element is a layer,\n","# and each layer is in turn a list of the layer variables (w, b)\n","\n","# first layer goes from n_input to n_hidden\n","w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n","                      name=\"w0\")\n","b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n","layers = [[w_input, b_input]]\n","\n","# all other hidden layers go from n_hidden to n_hidden\n","for layer in range(n_layers - 1):\n","    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n","                    name=\"w\" + str(layer+1))\n","    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n","    layers.append([w, b])\n","\n","# finally add the output layer\n","w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n","                    name=\"wout\")\n","b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n","layers.append([w_out, b_out])\n","\n","# flatten the layers to get a list of variables\n","all_variables = [variable for layer in layers for variable in layer]\n","\n","\n","def model_forward(inputs):\n","    x = inputs\n","    for w, b in layers[:-1]:\n","        x = tf.nn.relu(tf.matmul(x, w) + b)\n","    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n","\n","    return logits\n","\n","\n","lr = 0.1\n","train_steps = 2000\n","for step, (img_batch, lbl_batch) in enumerate(train_data):\n","    if step > train_steps:\n","        break\n","\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        logits = model_forward(img_batch)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=logits, labels=lbl_batch))\n","\n","    grads = tape.gradient(xent, all_variables)\n","    for grad, var in zip(grads, all_variables):\n","        var.assign_sub(lr*grad)\n","\n","    if not step % 100:\n","        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","test_preds = model_forward(test_images)\n","test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(acc))\n"]},{"cell_type":"markdown","metadata":{"id":"1ZrjBiweqkeh"},"source":["When the weight range changed, the accuracy improved considerably. But not the best. So we reduced the number of layers from 8 to 3."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvRf7sXpX1z6"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","# get the data\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","\n","def preprocess_images(images):\n","    return images.reshape(-1, 784).astype(np.float32) / 255\n","\n","\n","def preprocess_labels(labels):\n","    return labels.reshape(-1).astype(np.int32)\n","\n","\n","train_images = preprocess_images(train_images)\n","test_images = preprocess_images(test_images)\n","train_labels = preprocess_labels(train_labels)\n","test_labels = preprocess_labels(test_labels)\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n","#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n","\n","\n","# define the model first, from input to output\n","\n","# this is a super deep model, cool!\n","n_units = 100\n","n_layers = 3\n","w_range = 0.1\n","\n","# just set up a \"chain\" of hidden layers\n","# model is represented by a list where each element is a layer,\n","# and each layer is in turn a list of the layer variables (w, b)\n","\n","# first layer goes from n_input to n_hidden\n","w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n","                      name=\"w0\")\n","b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n","layers = [[w_input, b_input]]\n","\n","# all other hidden layers go from n_hidden to n_hidden\n","for layer in range(n_layers - 1):\n","    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n","                    name=\"w\" + str(layer+1))\n","    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n","    layers.append([w, b])\n","\n","# finally add the output layer\n","w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n","                    name=\"wout\")\n","b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n","layers.append([w_out, b_out])\n","\n","# flatten the layers to get a list of variables\n","all_variables = [variable for layer in layers for variable in layer]\n","\n","\n","def model_forward(inputs):\n","    x = inputs\n","    for w, b in layers[:-1]:\n","        x = tf.nn.relu(tf.matmul(x, w) + b)\n","    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n","\n","    return logits\n","\n","\n","lr = 0.1\n","train_steps = 2000\n","for step, (img_batch, lbl_batch) in enumerate(train_data):\n","    if step > train_steps:\n","        break\n","\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        logits = model_forward(img_batch)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=logits, labels=lbl_batch))\n","\n","    grads = tape.gradient(xent, all_variables)\n","    for grad, var in zip(grads, all_variables):\n","        var.assign_sub(lr*grad)\n","\n","    if not step % 100:\n","        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","test_preds = model_forward(test_images)\n","test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(acc))\n"]},{"cell_type":"markdown","metadata":{"id":"XvO87gcCqvTg"},"source":["When both weight range and number of layers were reduced, the model could perform very well."]},{"cell_type":"markdown","metadata":{"id":"mKGLqkYuYIBe"},"source":["# Fail 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50047,"status":"ok","timestamp":1697994908280,"user":{"displayName":"Minu Genty","userId":"09261842457608424196"},"user_tz":-120},"id":"m5hx2lnNYUHM","outputId":"b51cba91-00fc-41af-c86a-74aa010451ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss: 2.352381706237793 Accuracy: 0.0703125\n","Loss: 2.301762342453003 Accuracy: 0.140625\n","Loss: 2.300487995147705 Accuracy: 0.09375\n","Loss: 2.3106918334960938 Accuracy: 0.0859375\n","Loss: 2.3086981773376465 Accuracy: 0.140625\n","Loss: 2.3047871589660645 Accuracy: 0.109375\n","Loss: 2.3056468963623047 Accuracy: 0.109375\n","Loss: 2.3188700675964355 Accuracy: 0.109375\n","Loss: 2.3012189865112305 Accuracy: 0.109375\n","Loss: 2.3052403926849365 Accuracy: 0.1015625\n","Loss: 2.3072829246520996 Accuracy: 0.109375\n","Loss: 2.304764986038208 Accuracy: 0.0859375\n","Loss: 2.305140733718872 Accuracy: 0.140625\n","Loss: 2.306997776031494 Accuracy: 0.078125\n","Loss: 2.313633441925049 Accuracy: 0.09375\n","Loss: 2.305418014526367 Accuracy: 0.109375\n","Loss: 2.3226475715637207 Accuracy: 0.078125\n","Loss: 2.316695213317871 Accuracy: 0.09375\n","Loss: 2.3057918548583984 Accuracy: 0.09375\n","Loss: 2.3130481243133545 Accuracy: 0.09375\n","Loss: 2.3107738494873047 Accuracy: 0.0546875\n","Final test accuracy: 0.11349999904632568\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","# get the data\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","\n","def preprocess_images(images):\n","    return images.reshape(-1, 784).astype(np.float32) / 255\n","\n","\n","def preprocess_labels(labels):\n","    return labels.reshape(-1).astype(np.int32)\n","\n","\n","train_images = preprocess_images(train_images)\n","test_images = preprocess_images(test_images)\n","train_labels = preprocess_labels(train_labels)\n","test_labels = preprocess_labels(test_labels)\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n","#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n","\n","\n","# define the model first, from input to output\n","\n","# this is a super deep model, cool!\n","n_units = 100\n","n_layers = 8\n","w_range = 0.1\n","\n","# just set up a \"chain\" of hidden layers\n","# model is represented by a list where each element is a layer,\n","# and each layer is in turn a list of the layer variables (w, b)\n","\n","# first layer goes from n_input to n_hidden\n","w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n","                      name=\"w0\")\n","b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n","layers = [[w_input, b_input]]\n","\n","# all other hidden layers go from n_hidden to n_hidden\n","for layer in range(n_layers - 1):\n","    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n","                    name=\"w\" + str(layer+1))\n","    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n","    layers.append([w, b])\n","\n","# finally add the output layer\n","w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n","                    name=\"wout\")\n","b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n","layers.append([w_out, b_out])\n","\n","# flatten the layers to get a list of variables\n","all_variables = [variable for layer in layers for variable in layer]\n","\n","\n","def model_forward(inputs):\n","    x = inputs\n","    for w, b in layers[:-1]:\n","        x = tf.nn.sigmoid(tf.matmul(x, w) + b)\n","    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n","\n","    return logits\n","\n","\n","lr = 0.1\n","train_steps = 2000\n","for step, (img_batch, lbl_batch) in enumerate(train_data):\n","    if step > train_steps:\n","        break\n","\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        logits = model_forward(img_batch)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=logits, labels=lbl_batch))\n","\n","    grads = tape.gradient(xent, all_variables)\n","    for grad, var in zip(grads, all_variables):\n","        var.assign_sub(lr*grad)\n","\n","    if not step % 100:\n","        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","test_preds = model_forward(test_images)\n","test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(acc))\n"]},{"cell_type":"markdown","metadata":{"id":"DyNbZ60JXXBQ"},"source":["The low performance could be due to\n","\n","1.   number of layers\n","2.   activation function used\n","\n","We first tried reducing the number of layers to 3.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29073,"status":"ok","timestamp":1697995080834,"user":{"displayName":"Minu Genty","userId":"09261842457608424196"},"user_tz":-120},"id":"IZpGlz2jZB_P","outputId":"63537514-ab64-4027-c420-45ebd04b8075"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss: 2.344776153564453 Accuracy: 0.125\n","Loss: 2.3150441646575928 Accuracy: 0.1015625\n","Loss: 2.2965567111968994 Accuracy: 0.1484375\n","Loss: 2.309427261352539 Accuracy: 0.09375\n","Loss: 2.311738967895508 Accuracy: 0.109375\n","Loss: 2.289350986480713 Accuracy: 0.1484375\n","Loss: 2.2847259044647217 Accuracy: 0.109375\n","Loss: 2.3096697330474854 Accuracy: 0.1171875\n","Loss: 2.3044545650482178 Accuracy: 0.09375\n","Loss: 2.313081979751587 Accuracy: 0.1015625\n","Loss: 2.306049346923828 Accuracy: 0.078125\n","Loss: 2.301743268966675 Accuracy: 0.09375\n","Loss: 2.2939279079437256 Accuracy: 0.203125\n","Loss: 2.29081654548645 Accuracy: 0.1875\n","Loss: 2.299832582473755 Accuracy: 0.1328125\n","Loss: 2.2990283966064453 Accuracy: 0.109375\n","Loss: 2.3014068603515625 Accuracy: 0.234375\n","Loss: 2.276395320892334 Accuracy: 0.1640625\n","Loss: 2.270008087158203 Accuracy: 0.2421875\n","Loss: 2.234407424926758 Accuracy: 0.2890625\n","Loss: 2.224774122238159 Accuracy: 0.2265625\n","Final test accuracy: 0.20440000295639038\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","# get the data\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","\n","def preprocess_images(images):\n","    return images.reshape(-1, 784).astype(np.float32) / 255\n","\n","\n","def preprocess_labels(labels):\n","    return labels.reshape(-1).astype(np.int32)\n","\n","\n","train_images = preprocess_images(train_images)\n","test_images = preprocess_images(test_images)\n","train_labels = preprocess_labels(train_labels)\n","test_labels = preprocess_labels(test_labels)\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n","#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n","\n","\n","# define the model first, from input to output\n","\n","# this is a super deep model, cool!\n","n_units = 100\n","n_layers = 3\n","w_range = 0.1\n","\n","# just set up a \"chain\" of hidden layers\n","# model is represented by a list where each element is a layer,\n","# and each layer is in turn a list of the layer variables (w, b)\n","\n","# first layer goes from n_input to n_hidden\n","w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n","                      name=\"w0\")\n","b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n","layers = [[w_input, b_input]]\n","\n","# all other hidden layers go from n_hidden to n_hidden\n","for layer in range(n_layers - 1):\n","    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n","                    name=\"w\" + str(layer+1))\n","    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n","    layers.append([w, b])\n","\n","# finally add the output layer\n","w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n","                    name=\"wout\")\n","b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n","layers.append([w_out, b_out])\n","\n","# flatten the layers to get a list of variables\n","all_variables = [variable for layer in layers for variable in layer]\n","\n","\n","def model_forward(inputs):\n","    x = inputs\n","    for w, b in layers[:-1]:\n","        x = tf.nn.sigmoid(tf.matmul(x, w) + b)\n","    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n","\n","    return logits\n","\n","\n","lr = 0.1\n","train_steps = 2000\n","for step, (img_batch, lbl_batch) in enumerate(train_data):\n","    if step > train_steps:\n","        break\n","\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        logits = model_forward(img_batch)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=logits, labels=lbl_batch))\n","\n","    grads = tape.gradient(xent, all_variables)\n","    for grad, var in zip(grads, all_variables):\n","        var.assign_sub(lr*grad)\n","\n","    if not step % 100:\n","        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","test_preds = model_forward(test_images)\n","test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(acc))\n"]},{"cell_type":"markdown","metadata":{"id":"9-L7sqepw6RF"},"source":["The reduced number of layers didn't improve the performance much. So the next choice was to change the sigmoid activation function. It causes **vanishing gradient problem**. We tried ReLU instead."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29038,"status":"ok","timestamp":1697995127963,"user":{"displayName":"Minu Genty","userId":"09261842457608424196"},"user_tz":-120},"id":"eaRK80sCZN3A","outputId":"3f34cacc-9a5b-4f61-e899-dabb5cd57b87"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss: 2.294020175933838 Accuracy: 0.1328125\n","Loss: 0.9948858022689819 Accuracy: 0.7890625\n","Loss: 0.42462366819381714 Accuracy: 0.8984375\n","Loss: 0.5211051106452942 Accuracy: 0.859375\n","Loss: 0.2737477421760559 Accuracy: 0.921875\n","Loss: 0.38028883934020996 Accuracy: 0.8984375\n","Loss: 0.22292974591255188 Accuracy: 0.921875\n","Loss: 0.27285289764404297 Accuracy: 0.9375\n","Loss: 0.24349601566791534 Accuracy: 0.9296875\n","Loss: 0.2610745131969452 Accuracy: 0.9296875\n","Loss: 0.08229203522205353 Accuracy: 0.9765625\n","Loss: 0.14334313571453094 Accuracy: 0.9609375\n","Loss: 0.17306563258171082 Accuracy: 0.9609375\n","Loss: 0.16864192485809326 Accuracy: 0.9375\n","Loss: 0.0989251434803009 Accuracy: 0.96875\n","Loss: 0.15837225317955017 Accuracy: 0.9375\n","Loss: 0.0818743035197258 Accuracy: 0.96875\n","Loss: 0.16728579998016357 Accuracy: 0.9609375\n","Loss: 0.1514917016029358 Accuracy: 0.9609375\n","Loss: 0.24887192249298096 Accuracy: 0.9296875\n","Loss: 0.15030977129936218 Accuracy: 0.96875\n","Final test accuracy: 0.960099995136261\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","# get the data\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","\n","def preprocess_images(images):\n","    return images.reshape(-1, 784).astype(np.float32) / 255\n","\n","\n","def preprocess_labels(labels):\n","    return labels.reshape(-1).astype(np.int32)\n","\n","\n","train_images = preprocess_images(train_images)\n","test_images = preprocess_images(test_images)\n","train_labels = preprocess_labels(train_labels)\n","test_labels = preprocess_labels(test_labels)\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n","#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n","\n","\n","# define the model first, from input to output\n","\n","# this is a super deep model, cool!\n","n_units = 100\n","n_layers = 3\n","w_range = 0.1\n","\n","# just set up a \"chain\" of hidden layers\n","# model is represented by a list where each element is a layer,\n","# and each layer is in turn a list of the layer variables (w, b)\n","\n","# first layer goes from n_input to n_hidden\n","w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n","                      name=\"w0\")\n","b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n","layers = [[w_input, b_input]]\n","\n","# all other hidden layers go from n_hidden to n_hidden\n","for layer in range(n_layers - 1):\n","    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n","                    name=\"w\" + str(layer+1))\n","    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n","    layers.append([w, b])\n","\n","# finally add the output layer\n","w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n","                    name=\"wout\")\n","b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n","layers.append([w_out, b_out])\n","\n","# flatten the layers to get a list of variables\n","all_variables = [variable for layer in layers for variable in layer]\n","\n","\n","def model_forward(inputs):\n","    x = inputs\n","    for w, b in layers[:-1]:\n","        x = tf.nn.relu(tf.matmul(x, w) + b)\n","    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n","\n","    return logits\n","\n","\n","lr = 0.1\n","train_steps = 2000\n","for step, (img_batch, lbl_batch) in enumerate(train_data):\n","    if step > train_steps:\n","        break\n","\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        logits = model_forward(img_batch)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=logits, labels=lbl_batch))\n","\n","    grads = tape.gradient(xent, all_variables)\n","    for grad, var in zip(grads, all_variables):\n","        var.assign_sub(lr*grad)\n","\n","    if not step % 100:\n","        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","test_preds = model_forward(test_images)\n","test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(acc))\n"]},{"cell_type":"markdown","metadata":{"id":"SvY1ETWWxW5R"},"source":["Using ReLU improved the performance."]},{"cell_type":"markdown","metadata":{"id":"WV4Sh-o_ZSId"},"source":["# Fail 3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42321,"status":"ok","timestamp":1697995198353,"user":{"displayName":"Minu Genty","userId":"09261842457608424196"},"user_tz":-120},"id":"_NsfzchNZcoq","outputId":"9790a407-b0dc-4699-c0e4-fbd4f89c3ec1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss: 2.3025851249694824 Accuracy: 0.09375\n","Loss: 2.302135467529297 Accuracy: 0.0859375\n","Loss: 2.3048126697540283 Accuracy: 0.125\n","Loss: 2.301588535308838 Accuracy: 0.125\n","Loss: 2.304137945175171 Accuracy: 0.09375\n","Loss: 2.305006980895996 Accuracy: 0.1171875\n","Loss: 2.295255422592163 Accuracy: 0.140625\n","Loss: 2.3096563816070557 Accuracy: 0.1015625\n","Loss: 2.307626247406006 Accuracy: 0.125\n","Loss: 2.304675579071045 Accuracy: 0.109375\n","Loss: 2.294935703277588 Accuracy: 0.1796875\n","Loss: 2.304737091064453 Accuracy: 0.109375\n","Loss: 2.304884433746338 Accuracy: 0.1015625\n","Loss: 2.295850992202759 Accuracy: 0.140625\n","Loss: 2.3044605255126953 Accuracy: 0.1015625\n","Loss: 2.300802230834961 Accuracy: 0.109375\n","Loss: 2.290827751159668 Accuracy: 0.1640625\n","Loss: 2.2948594093322754 Accuracy: 0.1171875\n","Loss: 2.2937498092651367 Accuracy: 0.109375\n","Loss: 2.300554037094116 Accuracy: 0.0859375\n","Loss: 2.2955498695373535 Accuracy: 0.1484375\n","Final test accuracy: 0.11349999904632568\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","# get the data\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","\n","def preprocess_images(images):\n","    return images.reshape(-1, 784).astype(np.float32) / 255\n","\n","\n","def preprocess_labels(labels):\n","    return labels.reshape(-1).astype(np.int32)\n","\n","\n","train_images = preprocess_images(train_images)\n","test_images = preprocess_images(test_images)\n","train_labels = preprocess_labels(train_labels)\n","test_labels = preprocess_labels(test_labels)\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n","#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n","\n","\n","# define the model first, from input to output\n","\n","# uhm, maybe don't use that many layers actually. 2 is fine!\n","n_units = 100\n","n_layers = 2\n","w_range = 0.1\n","\n","# just set up a \"chain\" of hidden layers\n","# model is represented by a list where each element is a layer,\n","# and each layer is in turn a list of the layer variables (w, b)\n","\n","# first layer goes from n_input to n_hidden\n","w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, 0.),\n","                      name=\"w0\")\n","b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n","layers = [[w_input, b_input]]\n","\n","# all other hidden layers go from n_hidden to n_hidden\n","for layer in range(n_layers - 1):\n","    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, 0.),\n","                    name=\"w\" + str(layer+1))\n","    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n","    layers.append([w, b])\n","\n","# finally add the output layer\n","w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, 0.),\n","                    name=\"wout\")\n","b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n","layers.append([w_out, b_out])\n","\n","# flatten the layers to get a list of variables\n","all_variables = [variable for layer in layers for variable in layer]\n","\n","\n","def model_forward(inputs):\n","    x = inputs\n","    for w, b in layers[:-1]:\n","        x = tf.nn.relu(tf.matmul(x, w) + b)\n","    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n","\n","    return logits\n","\n","\n","lr = 0.1\n","train_steps = 2000\n","for step, (img_batch, lbl_batch) in enumerate(train_data):\n","    if step > train_steps:\n","        break\n","\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        logits = model_forward(img_batch)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=logits, labels=lbl_batch))\n","\n","    grads = tape.gradient(xent, all_variables)\n","    for grad, var in zip(grads, all_variables):\n","        var.assign_sub(lr*grad)\n","\n","    if not step % 100:\n","        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","test_preds = model_forward(test_images)\n","test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(acc))\n"]},{"cell_type":"markdown","metadata":{"id":"aZj5nG8B3mt5"},"source":["The low performance is due to the weight initialization. The weight range from -0.1 to 0 and the activation function used is ReLU. When a negative value is fed to ReLU, it will convert it to zero, by which no weights are learned by the network.\n","\n","We tried to change the weight range and the network performance increased."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42122,"status":"ok","timestamp":1697995356203,"user":{"displayName":"Minu Genty","userId":"09261842457608424196"},"user_tz":-120},"id":"66t3m0iwZ9mS","outputId":"423839e4-6d4a-414f-e7ee-c38f8cff7f82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss: 2.291508674621582 Accuracy: 0.125\n","Loss: 0.6187820434570312 Accuracy: 0.796875\n","Loss: 0.4856111705303192 Accuracy: 0.8828125\n","Loss: 0.48460501432418823 Accuracy: 0.8359375\n","Loss: 0.42738106846809387 Accuracy: 0.84375\n","Loss: 0.28208041191101074 Accuracy: 0.90625\n","Loss: 0.2716045379638672 Accuracy: 0.90625\n","Loss: 0.2966870665550232 Accuracy: 0.9375\n","Loss: 0.29672661423683167 Accuracy: 0.9140625\n","Loss: 0.33935150504112244 Accuracy: 0.8984375\n","Loss: 0.15933004021644592 Accuracy: 0.9453125\n","Loss: 0.20918746292591095 Accuracy: 0.9140625\n","Loss: 0.255983829498291 Accuracy: 0.9140625\n","Loss: 0.08910238742828369 Accuracy: 0.984375\n","Loss: 0.24497516453266144 Accuracy: 0.9296875\n","Loss: 0.13901779055595398 Accuracy: 0.953125\n","Loss: 0.11571440100669861 Accuracy: 0.96875\n","Loss: 0.20443148910999298 Accuracy: 0.953125\n","Loss: 0.11137984693050385 Accuracy: 0.96875\n","Loss: 0.1868687868118286 Accuracy: 0.9375\n","Loss: 0.12050947546958923 Accuracy: 0.9921875\n","Final test accuracy: 0.9567999839782715\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","# get the data\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","\n","def preprocess_images(images):\n","    return images.reshape(-1, 784).astype(np.float32) / 255\n","\n","\n","def preprocess_labels(labels):\n","    return labels.reshape(-1).astype(np.int32)\n","\n","\n","train_images = preprocess_images(train_images)\n","test_images = preprocess_images(test_images)\n","train_labels = preprocess_labels(train_labels)\n","test_labels = preprocess_labels(test_labels)\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n","#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n","\n","\n","# define the model first, from input to output\n","\n","# uhm, maybe don't use that many layers actually. 2 is fine!\n","n_units = 100\n","n_layers = 2\n","w_range = 0.1\n","\n","# just set up a \"chain\" of hidden layers\n","# model is represented by a list where each element is a layer,\n","# and each layer is in turn a list of the layer variables (w, b)\n","\n","# first layer goes from n_input to n_hidden\n","w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n","                      name=\"w0\")\n","b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n","layers = [[w_input, b_input]]\n","\n","# all other hidden layers go from n_hidden to n_hidden\n","for layer in range(n_layers - 1):\n","    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n","                    name=\"w\" + str(layer+1))\n","    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n","    layers.append([w, b])\n","\n","# finally add the output layer\n","w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n","                    name=\"wout\")\n","b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n","layers.append([w_out, b_out])\n","\n","# flatten the layers to get a list of variables\n","all_variables = [variable for layer in layers for variable in layer]\n","\n","\n","def model_forward(inputs):\n","    x = inputs\n","    for w, b in layers[:-1]:\n","        x = tf.nn.relu(tf.matmul(x, w) + b)\n","    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n","\n","    return logits\n","\n","\n","lr = 0.1\n","train_steps = 2000\n","for step, (img_batch, lbl_batch) in enumerate(train_data):\n","    if step > train_steps:\n","        break\n","\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        logits = model_forward(img_batch)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=logits, labels=lbl_batch))\n","\n","    grads = tape.gradient(xent, all_variables)\n","    for grad, var in zip(grads, all_variables):\n","        var.assign_sub(lr*grad)\n","\n","    if not step % 100:\n","        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","test_preds = model_forward(test_images)\n","test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(acc))\n"]},{"cell_type":"markdown","metadata":{"id":"ZKuc8VvQaLlX"},"source":["# Fail 4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42328,"status":"ok","timestamp":1697995462017,"user":{"displayName":"Minu Genty","userId":"09261842457608424196"},"user_tz":-120},"id":"vKJQGMToaNlf","outputId":"e8778d91-97d2-45ff-87e3-b4110fbaa79e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss: 2.819425344467163 Accuracy: 0.0703125\n","Loss: 2.2856688499450684 Accuracy: 0.109375\n","Loss: 2.3138582706451416 Accuracy: 0.09375\n","Loss: 2.315075397491455 Accuracy: 0.1953125\n","Loss: 2.243828535079956 Accuracy: 0.1328125\n","Loss: 2.205674648284912 Accuracy: 0.21875\n","Loss: 2.151824712753296 Accuracy: 0.1953125\n","Loss: 2.287320613861084 Accuracy: 0.1328125\n","Loss: 2.247054100036621 Accuracy: 0.09375\n","Loss: 2.1894426345825195 Accuracy: 0.1796875\n","Loss: 2.1548807621002197 Accuracy: 0.2265625\n","Loss: 2.1560263633728027 Accuracy: 0.234375\n","Loss: 2.1825413703918457 Accuracy: 0.1796875\n","Loss: 2.0566048622131348 Accuracy: 0.2734375\n","Loss: 2.1449694633483887 Accuracy: 0.2265625\n","Loss: 2.1286816596984863 Accuracy: 0.21875\n","Loss: 2.0888171195983887 Accuracy: 0.234375\n","Loss: 2.1137797832489014 Accuracy: 0.1953125\n","Loss: 2.032057523727417 Accuracy: 0.3125\n","Loss: 2.158693790435791 Accuracy: 0.1796875\n","Loss: 2.111768960952759 Accuracy: 0.21875\n","Final test accuracy: 0.6211000084877014\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","# get the data\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","\n","def preprocess_images(images):\n","    return images.reshape(-1, 784).astype(np.float32) / 255\n","\n","\n","def preprocess_labels(labels):\n","    return labels.reshape(-1).astype(np.int32)\n","\n","\n","train_images = preprocess_images(train_images)\n","test_images = preprocess_images(test_images)\n","train_labels = preprocess_labels(train_labels)\n","test_labels = preprocess_labels(test_labels)\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n","#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n","\n","\n","# define the model first, from input to output\n","\n","# 2 layers again\n","n_units = 100\n","n_layers = 2\n","w_range = 0.1\n","\n","# just set up a \"chain\" of hidden layers\n","# model is represented by a list where each element is a layer,\n","# and each layer is in turn a list of the layer variables (w, b)\n","\n","# first layer goes from n_input to n_hidden\n","w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n","                      name=\"w0\")\n","b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n","layers = [[w_input, b_input]]\n","\n","# all other hidden layers go from n_hidden to n_hidden\n","for layer in range(n_layers - 1):\n","    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n","                    name=\"w\" + str(layer+1))\n","    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n","    layers.append([w, b])\n","\n","# finally add the output layer\n","w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n","                    name=\"wout\")\n","b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n","layers.append([w_out, b_out])\n","\n","# flatten the layers to get a list of variables\n","all_variables = [variable for layer in layers for variable in layer]\n","\n","\n","def model_forward(inputs):\n","    x = inputs\n","    for w, b in layers[:-1]:\n","        x = tf.nn.relu(tf.matmul(x, w) + b)\n","    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n","\n","    return logits\n","\n","\n","lr = 0.1\n","train_steps = 2000\n","for step, (img_batch, lbl_batch) in enumerate(train_data):\n","    if step > train_steps:\n","        break\n","\n","    # I hear that adding noise to the inputs improves generalization!\n","    img_batch += tf.random.normal(tf.shape(img_batch), stddev=4.)\n","\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        logits = model_forward(img_batch)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=logits, labels=lbl_batch))\n","\n","    grads = tape.gradient(xent, all_variables)\n","    for grad, var in zip(grads, all_variables):\n","        var.assign_sub(lr*grad)\n","\n","    if not step % 100:\n","        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","test_preds = model_forward(test_images)\n","test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(acc))\n"]},{"cell_type":"markdown","metadata":{"id":"_uruGBNp4TOy"},"source":["The reason for fail is due to the noise introduced in the network. Even though adding noise helpsin generalizing the model, here the noise has a standard deviation of 4, which means, the data is more spread and more randomness is introduced. The network won't be able to learn anything.\n","\n","We tried to reduce the standard deviation to 1."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30166,"status":"ok","timestamp":1697995987400,"user":{"displayName":"Minu Genty","userId":"09261842457608424196"},"user_tz":-120},"id":"O23ZMW0-chFS","outputId":"56f1f6bf-7816-4046-d43c-c1fcdf5ed482"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss: 2.342329978942871 Accuracy: 0.109375\n","Loss: 1.6519036293029785 Accuracy: 0.3984375\n","Loss: 1.2707027196884155 Accuracy: 0.5625\n","Loss: 1.0570746660232544 Accuracy: 0.671875\n","Loss: 0.9510445594787598 Accuracy: 0.640625\n","Loss: 0.8527084589004517 Accuracy: 0.6796875\n","Loss: 1.0074198246002197 Accuracy: 0.6875\n","Loss: 0.8021036982536316 Accuracy: 0.703125\n","Loss: 0.9749041795730591 Accuracy: 0.6640625\n","Loss: 0.8439110517501831 Accuracy: 0.71875\n","Loss: 0.6273705363273621 Accuracy: 0.765625\n","Loss: 0.9150989055633545 Accuracy: 0.671875\n","Loss: 0.7954504489898682 Accuracy: 0.7421875\n","Loss: 0.8161919713020325 Accuracy: 0.6796875\n","Loss: 0.8832588791847229 Accuracy: 0.71875\n","Loss: 0.7474455833435059 Accuracy: 0.765625\n","Loss: 0.7954943180084229 Accuracy: 0.734375\n","Loss: 0.6909277439117432 Accuracy: 0.78125\n","Loss: 0.6738259792327881 Accuracy: 0.78125\n","Loss: 0.8695346713066101 Accuracy: 0.703125\n","Loss: 0.7154517769813538 Accuracy: 0.7578125\n","Final test accuracy: 0.9164000153541565\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","# get the data\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","\n","def preprocess_images(images):\n","    return images.reshape(-1, 784).astype(np.float32) / 255\n","\n","\n","def preprocess_labels(labels):\n","    return labels.reshape(-1).astype(np.int32)\n","\n","\n","train_images = preprocess_images(train_images)\n","test_images = preprocess_images(test_images)\n","train_labels = preprocess_labels(train_labels)\n","test_labels = preprocess_labels(test_labels)\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n","#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n","\n","\n","# define the model first, from input to output\n","\n","# 2 layers again\n","n_units = 100\n","n_layers = 2\n","w_range = 0.1\n","\n","# just set up a \"chain\" of hidden layers\n","# model is represented by a list where each element is a layer,\n","# and each layer is in turn a list of the layer variables (w, b)\n","\n","# first layer goes from n_input to n_hidden\n","w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n","                      name=\"w0\")\n","b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n","layers = [[w_input, b_input]]\n","\n","# all other hidden layers go from n_hidden to n_hidden\n","for layer in range(n_layers - 1):\n","    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n","                    name=\"w\" + str(layer+1))\n","    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n","    layers.append([w, b])\n","\n","# finally add the output layer\n","w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n","                    name=\"wout\")\n","b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n","layers.append([w_out, b_out])\n","\n","# flatten the layers to get a list of variables\n","all_variables = [variable for layer in layers for variable in layer]\n","\n","\n","def model_forward(inputs):\n","    x = inputs\n","    for w, b in layers[:-1]:\n","        x = tf.nn.relu(tf.matmul(x, w) + b)\n","    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n","\n","    return logits\n","\n","\n","lr = 0.1\n","train_steps = 2000\n","for step, (img_batch, lbl_batch) in enumerate(train_data):\n","    if step > train_steps:\n","        break\n","\n","    # I hear that adding noise to the inputs improves generalization!\n","    img_batch += tf.random.normal(tf.shape(img_batch), stddev=1.)\n","\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        logits = model_forward(img_batch)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=logits, labels=lbl_batch))\n","\n","    grads = tape.gradient(xent, all_variables)\n","    for grad, var in zip(grads, all_variables):\n","        var.assign_sub(lr*grad)\n","\n","    if not step % 100:\n","        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","test_preds = model_forward(test_images)\n","test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(acc))\n"]},{"cell_type":"markdown","metadata":{"id":"S_2M14b25CZx"},"source":["The reduced standard deviation 1 of the noise has improved the performance of the network. We reduces the standard deviation further to 0.5 and that resulted in much improved performance."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33124,"status":"ok","timestamp":1697996039622,"user":{"displayName":"Minu Genty","userId":"09261842457608424196"},"user_tz":-120},"id":"HIG6QPRwcso3","outputId":"06ce0ba2-7fef-4960-8c34-a1bcd0b881c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss: 2.3102447986602783 Accuracy: 0.0859375\n","Loss: 1.1272873878479004 Accuracy: 0.671875\n","Loss: 0.6498242616653442 Accuracy: 0.78125\n","Loss: 0.6847140789031982 Accuracy: 0.7578125\n","Loss: 0.44196680188179016 Accuracy: 0.828125\n","Loss: 0.47915536165237427 Accuracy: 0.8125\n","Loss: 0.5217883586883545 Accuracy: 0.828125\n","Loss: 0.5374540686607361 Accuracy: 0.828125\n","Loss: 0.4636618196964264 Accuracy: 0.828125\n","Loss: 0.34883445501327515 Accuracy: 0.890625\n","Loss: 0.4403694272041321 Accuracy: 0.8203125\n","Loss: 0.36558669805526733 Accuracy: 0.890625\n","Loss: 0.39549750089645386 Accuracy: 0.8828125\n","Loss: 0.3721521198749542 Accuracy: 0.8359375\n","Loss: 0.3258657157421112 Accuracy: 0.90625\n","Loss: 0.4368760883808136 Accuracy: 0.875\n","Loss: 0.4760374128818512 Accuracy: 0.8671875\n","Loss: 0.2596156597137451 Accuracy: 0.890625\n","Loss: 0.28435051441192627 Accuracy: 0.8984375\n","Loss: 0.2324698269367218 Accuracy: 0.921875\n","Loss: 0.4001287817955017 Accuracy: 0.8515625\n","Final test accuracy: 0.9480000138282776\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","# get the data\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","\n","def preprocess_images(images):\n","    return images.reshape(-1, 784).astype(np.float32) / 255\n","\n","\n","def preprocess_labels(labels):\n","    return labels.reshape(-1).astype(np.int32)\n","\n","\n","train_images = preprocess_images(train_images)\n","test_images = preprocess_images(test_images)\n","train_labels = preprocess_labels(train_labels)\n","test_labels = preprocess_labels(test_labels)\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n","#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n","\n","\n","# define the model first, from input to output\n","\n","# 2 layers again\n","n_units = 100\n","n_layers = 2\n","w_range = 0.1\n","\n","# just set up a \"chain\" of hidden layers\n","# model is represented by a list where each element is a layer,\n","# and each layer is in turn a list of the layer variables (w, b)\n","\n","# first layer goes from n_input to n_hidden\n","w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n","                      name=\"w0\")\n","b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n","layers = [[w_input, b_input]]\n","\n","# all other hidden layers go from n_hidden to n_hidden\n","for layer in range(n_layers - 1):\n","    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n","                    name=\"w\" + str(layer+1))\n","    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n","    layers.append([w, b])\n","\n","# finally add the output layer\n","w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n","                    name=\"wout\")\n","b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n","layers.append([w_out, b_out])\n","\n","# flatten the layers to get a list of variables\n","all_variables = [variable for layer in layers for variable in layer]\n","\n","\n","def model_forward(inputs):\n","    x = inputs\n","    for w, b in layers[:-1]:\n","        x = tf.nn.relu(tf.matmul(x, w) + b)\n","    logits = tf.matmul(x, layers[-1][0]) + layers[-1][1]\n","\n","    return logits\n","\n","\n","lr = 0.1\n","train_steps = 2000\n","for step, (img_batch, lbl_batch) in enumerate(train_data):\n","    if step > train_steps:\n","        break\n","\n","    # I hear that adding noise to the inputs improves generalization!\n","    img_batch += tf.random.normal(tf.shape(img_batch), stddev=.5)\n","\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        logits = model_forward(img_batch)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=logits, labels=lbl_batch))\n","\n","    grads = tape.gradient(xent, all_variables)\n","    for grad, var in zip(grads, all_variables):\n","        var.assign_sub(lr*grad)\n","\n","    if not step % 100:\n","        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","test_preds = model_forward(test_images)\n","test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(acc))\n"]},{"cell_type":"markdown","metadata":{"id":"CY_fqhmhcw94"},"source":["# Fail 5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42038,"status":"ok","timestamp":1697996101494,"user":{"displayName":"Minu Genty","userId":"09261842457608424196"},"user_tz":-120},"id":"Qw4AFbPEc3Ez","outputId":"92dc4e56-61dd-4f8f-d55d-ab8288430cc4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss: 2.3011035919189453 Accuracy: 0.09375\n","Loss: 2.293238639831543 Accuracy: 0.25\n","Loss: 2.265500545501709 Accuracy: 0.2421875\n","Loss: 2.192882537841797 Accuracy: 0.2578125\n","Loss: 2.0841286182403564 Accuracy: 0.4921875\n","Loss: 1.8973329067230225 Accuracy: 0.625\n","Loss: 1.8143310546875 Accuracy: 0.6640625\n","Loss: 1.8326491117477417 Accuracy: 0.640625\n","Loss: 1.7422444820404053 Accuracy: 0.7578125\n","Loss: 1.6693832874298096 Accuracy: 0.84375\n","Loss: 1.6791982650756836 Accuracy: 0.8125\n","Loss: 1.604892373085022 Accuracy: 0.8828125\n","Loss: 1.5751311779022217 Accuracy: 0.921875\n","Loss: 1.6142115592956543 Accuracy: 0.875\n","Loss: 1.5886831283569336 Accuracy: 0.8984375\n","Loss: 1.5818225145339966 Accuracy: 0.8984375\n","Loss: 1.585078477859497 Accuracy: 0.8828125\n","Loss: 1.6096453666687012 Accuracy: 0.8671875\n","Loss: 1.5533976554870605 Accuracy: 0.921875\n","Loss: 1.5722808837890625 Accuracy: 0.8984375\n","Loss: 1.5631394386291504 Accuracy: 0.90625\n","Final test accuracy: 0.9065999984741211\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","\n","# get the data\n","(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","\n","def preprocess_images(images):\n","    return images.reshape(-1, 784).astype(np.float32) / 255\n","\n","\n","def preprocess_labels(labels):\n","    return labels.reshape(-1).astype(np.int32)\n","\n","\n","train_images = preprocess_images(train_images)\n","test_images = preprocess_images(test_images)\n","train_labels = preprocess_labels(train_labels)\n","test_labels = preprocess_labels(test_labels)\n","\n","train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(60000).batch(128).repeat()\n","#test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(128)\n","\n","\n","# define the model first, from input to output\n","\n","# 2 layers again\n","n_units = 100\n","n_layers = 2\n","w_range = 0.1\n","\n","# just set up a \"chain\" of hidden layers\n","# model is represented by a list where each element is a layer,\n","# and each layer is in turn a list of the layer variables (w, b)\n","\n","# first layer goes from n_input to n_hidden\n","w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, w_range),\n","                      name=\"w0\")\n","b_input = tf.Variable(tf.zeros(n_units), name=\"b0\")\n","layers = [[w_input, b_input]]\n","\n","# all other hidden layers go from n_hidden to n_hidden\n","for layer in range(n_layers - 1):\n","    w = tf.Variable(tf.random.uniform([n_units, n_units], -w_range, w_range),\n","                    name=\"w\" + str(layer+1))\n","    b = tf.Variable(tf.zeros(n_units), name=\"b\" + str(layer+1))\n","    layers.append([w, b])\n","\n","# finally add the output layer\n","w_out = tf.Variable(tf.random.uniform([n_units, 10], -w_range, w_range),\n","                    name=\"wout\")\n","b_out = tf.Variable(tf.zeros(10), name=\"bout\")\n","layers.append([w_out, b_out])\n","\n","# flatten the layers to get a list of variables\n","all_variables = [variable for layer in layers for variable in layer]\n","\n","\n","def model_forward(inputs):\n","    x = inputs\n","    for w, b in layers[:-1]:\n","        x = tf.nn.relu(tf.matmul(x, w) + b)\n","    # finally, the softmax classification output layer :)))\n","    logits = tf.nn.softmax(tf.matmul(x, layers[-1][0]) + layers[-1][1])\n","\n","    return logits\n","\n","\n","lr = 0.1\n","train_steps = 2000\n","for step, (img_batch, lbl_batch) in enumerate(train_data):\n","    if step > train_steps:\n","        break\n","\n","    with tf.GradientTape() as tape:\n","        # here we just run all the layers in sequence via a for-loop\n","        logits = model_forward(img_batch)\n","        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n","            logits=logits, labels=lbl_batch))\n","\n","    grads = tape.gradient(xent, all_variables)\n","    for grad, var in zip(grads, all_variables):\n","        var.assign_sub(lr*grad)\n","\n","    if not step % 100:\n","        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n","        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n","        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n","\n","\n","test_preds = model_forward(test_images)\n","test_preds = tf.argmax(test_preds, axis=1, output_type=tf.int32)\n","acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels), tf.float32))\n","print(\"Final test accuracy: {}\".format(acc))\n"]},{"cell_type":"markdown","metadata":{"id":"aDXwDB7BXKAr"},"source":["The fail is due to the softmax used in the output layer. The network performance is not too low because all the hidden layers used ReLU while training. The softmax is only applied to the output layer, which affected the performance slightly.\n","\n","Softmax could cause vanishing gradient problem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tCJGj7OB5mSu"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["QoQfKPPoXPvv","mKGLqkYuYIBe","WV4Sh-o_ZSId","ZKuc8VvQaLlX","CY_fqhmhcw94"],"provenance":[{"file_id":"1n0IPTncD9cD7FthhCMkpuuo1PQZTkCmM","timestamp":1698129574388}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
